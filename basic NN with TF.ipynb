{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import bson\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.data import imread\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array, Iterator, ImageDataGenerator\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LOADING THE EXAMPLE DATA SET\n",
    "\n",
    "data = bson.decode_file_iter(open('train_example.bson', 'rb'))\n",
    "ids = []\n",
    "pics = []\n",
    "prod_to_category = dict()\n",
    "\n",
    "for c, d in enumerate(data):\n",
    "    product_id = d['_id']\n",
    "    category_id = d['category_id'] # This won't be in Test data\n",
    "    prod_to_category[product_id] = category_id\n",
    "    for e, pic in enumerate(d['imgs']):\n",
    "        ids.append(d['category_id'])\n",
    "        pics.append(imread(io.BytesIO(pic['picture'])))\n",
    "        picture = imread(io.BytesIO(pic['picture']))\n",
    "        # do something with the picture, etc\n",
    "\n",
    "prod_to_category = pd.DataFrame.from_dict(prod_to_category, orient='index')\n",
    "prod_to_category.index.name = '_id'\n",
    "prod_to_category.rename(columns={0: 'category_id'}, inplace=True)\n",
    "\n",
    "ids = np.asarray(ids)\n",
    "\n",
    "nppics = np.asarray(pics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "onehot_ids = enc.fit_transform(ids.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9898873 images belonging to 5270 classes.\n",
      "Found 2472420 images belonging to 5270 classes.\n"
     ]
    }
   ],
   "source": [
    "#LOAD THE REAL DATA WITH KERAS\n",
    "# I DON'T REALLY KNOW HOW THIS WORKS, BUT IT DOES\n",
    "\n",
    "def make_category_tables():\n",
    "    cat2idx = {}\n",
    "    idx2cat = {}\n",
    "    for ir in categories_df.itertuples():\n",
    "        category_id = ir[0]\n",
    "        category_idx = ir[4]\n",
    "        cat2idx[category_id] = category_idx\n",
    "        idx2cat[category_idx] = category_id\n",
    "    return cat2idx, idx2cat\n",
    "\n",
    "class BSONIterator(Iterator):\n",
    "    def __init__(self, bson_file, images_df, offsets_df, num_class,\n",
    "                 image_data_generator, target_size=(180, 180), with_labels=True,\n",
    "                 batch_size=32, shuffle=False, seed=None):\n",
    "\n",
    "        self.file = bson_file\n",
    "        self.images_df = images_df\n",
    "        self.offsets_df = offsets_df\n",
    "        self.with_labels = with_labels\n",
    "        self.samples = len(images_df)\n",
    "        self.num_class = num_class\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.target_size = tuple(target_size)\n",
    "        self.image_shape = self.target_size + (3,)\n",
    "\n",
    "        print(\"Found %d images belonging to %d classes.\" % (self.samples, self.num_class))\n",
    "\n",
    "        super(BSONIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        if self.with_labels:\n",
    "            batch_y = np.zeros((len(batch_x), self.num_class), dtype=K.floatx())\n",
    "            #batch_y = np.zeros((len(batch_x), 49), dtype=K.floatx())\n",
    "\n",
    "            #batch_y = np.zeros(len(batch_x), dtype=K.floatx())\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            # Protect file and dataframe access with a lock.\n",
    "            with self.lock:\n",
    "                image_row = self.images_df.iloc[j]\n",
    "                product_id = image_row[\"product_id\"]\n",
    "                offset_row = self.offsets_df.loc[product_id]\n",
    "\n",
    "                # Read this product's data from the BSON file.\n",
    "                self.file.seek(offset_row[\"offset\"])\n",
    "                item_data = self.file.read(offset_row[\"length\"])\n",
    "\n",
    "            # Grab the image from the product.\n",
    "            item = bson.BSON.decode(item_data)\n",
    "            img_idx = image_row[\"img_idx\"]\n",
    "            bson_img = item[\"imgs\"][img_idx][\"picture\"]\n",
    "\n",
    "            # Preprocess the image.\n",
    "            img = load_img(io.BytesIO(bson_img), target_size=self.target_size)\n",
    "            x = img_to_array(img)\n",
    "            x = self.image_data_generator.random_transform(x)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "\n",
    "            # Add the image and the label to the batch (one-hot encoded).\n",
    "            batch_x[i] = x\n",
    "            if self.with_labels:\n",
    "                #cat1_idx = le.transform(nt.loc[image_row[\"category_idx\"]]['category_level1'])\n",
    "                #batch_y[i, cat1_idx] = 1\n",
    "                batch_y[i, image_row[\"category_idx\"]] = 1\n",
    "                #batch_y[i] = image_row[\"category_idx\"]\n",
    "\n",
    "        if self.with_labels:\n",
    "            return batch_x, batch_y\n",
    "        else:\n",
    "            return batch_x\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "train_bson_path = \"train.bson\"\n",
    "\n",
    "\n",
    "    \n",
    "categories_df = pd.read_csv(\"categories.csv\", index_col=0)\n",
    "cat2idx, idx2cat = make_category_tables()\n",
    "\n",
    "train_offsets_df = pd.read_csv(\"train_offsets.csv\", index_col=0)\n",
    "train_images_df = pd.read_csv(\"train_images.csv\", index_col=0)\n",
    "val_images_df = pd.read_csv(\"val_images.csv\", index_col=0)\n",
    "\n",
    "train_bson_file = open(train_bson_path, \"rb\")\n",
    "\n",
    "num_classes = 5270\n",
    "num_train_images = len(train_images_df)\n",
    "num_val_images = len(val_images_df)\n",
    "batch_size = 128\n",
    "\n",
    "# Tip: use ImageDataGenerator for data augmentation and preprocessing.\n",
    "train_datagen = ImageDataGenerator()\n",
    "train_gen = BSONIterator(train_bson_file, train_images_df, train_offsets_df, \n",
    "                         num_classes, train_datagen, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_gen = BSONIterator(train_bson_file, val_images_df, train_offsets_df,\n",
    "                       num_classes, val_datagen, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time bx, by = next(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_tot = np.zeros(5270, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_tot += np.sum(by, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_4x4(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 4, 4, 1],\n",
    "                        strides=[1, 4, 4, 1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1],\n",
    "                        strides=[1, 3, 3, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 0.0; 0.02428842504743833\n",
      "after 100 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 0.0; 2.4531309297912713\n",
      "after 200 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 1.0; 4.881973434535104\n",
      "after 300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 1.0; 7.310815939278937\n",
      "after 400 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 2.0; 9.739658444022771\n",
      "after 500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 2.0; 12.168500948766603\n",
      "after 600 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 2.0; 14.597343453510437\n",
      "after 700 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 3.0; 17.02618595825427\n",
      "after 800 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 3.0; 19.4550284629981\n",
      "after 900 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 3.0; 21.883870967741935\n",
      "after 1000 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 4.0; 24.31271347248577\n",
      "after 1100 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 4.0; 26.741555977229602\n",
      "after 1200 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 5.0; 29.170398481973436\n",
      "after 1300 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 5.0; 31.599240986717266\n",
      "after 1400 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 5.0; 34.0280834914611\n",
      "after 1500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 6.0; 36.45692599620493\n",
      "after 1600 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 6.0; 38.885768500948764\n",
      "after 1700 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 6.0; 41.3146110056926\n",
      "after 1800 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 7.0; 43.74345351043643\n",
      "after 1900 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 7.0; 46.172296015180265\n",
      "after 2000 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 8.0; 48.6011385199241\n",
      "after 2100 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 8.0; 51.02998102466793\n",
      "after 2200 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 8.0; 53.45882352941177\n",
      "after 2300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 9.0; 55.8876660341556\n",
      "after 2400 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 9.0; 58.316508538899434\n",
      "after 2500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 10.0; 60.74535104364326\n",
      "after 2600 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 10.0; 63.174193548387095\n",
      "after 2700 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 10.0; 65.60303605313094\n",
      "after 2800 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 11.0; 68.03187855787476\n",
      "after 2900 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 11.0; 70.46072106261859\n",
      "after 3000 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 11.0; 72.88956356736243\n",
      "after 3100 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 12.0; 75.31840607210626\n",
      "after 3200 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 12.0; 77.7472485768501\n",
      "after 3300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 12.0; 80.17609108159392\n",
      "after 3400 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 13.0; 82.60493358633776\n",
      "after 3500 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 13.0; 85.03377609108159\n",
      "after 3600 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 13.0; 87.46261859582543\n",
      "after 3700 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 14.0; 89.89146110056926\n",
      "after 3800 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 14.0; 92.32030360531309\n",
      "after 3900 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 15.0; 94.74914611005693\n",
      "after 4000 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 15.0; 97.17798861480075\n",
      "after 4100 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 15.0; 99.6068311195446\n",
      "after 4200 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 16.0; 102.03567362428842\n",
      "after 4300 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 16.0; 104.46451612903226\n",
      "after 4400 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 17.0; 106.89335863377609\n",
      "after 4500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 17.0; 109.32220113851993\n",
      "after 4600 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 17.0; 111.75104364326376\n",
      "after 4700 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 18.0; 114.1798861480076\n",
      "after 4800 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 18.0; 116.60872865275142\n",
      "after 4900 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 18.0; 119.03757115749525\n",
      "after 5000 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 19.0; 121.46641366223909\n",
      "after 5100 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 19.0; 123.89525616698292\n",
      "after 5200 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 19.0; 126.32409867172676\n",
      "after 5300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 20.0; 128.75294117647059\n",
      "after 5400 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 20.0; 131.18178368121443\n",
      "after 5500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 21.0; 133.61062618595827\n",
      "after 5600 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 21.0; 136.03946869070208\n",
      "after 5700 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 21.0; 138.46831119544592\n",
      "after 5800 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 22.0; 140.89715370018976\n",
      "after 5900 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 22.0; 143.32599620493357\n",
      "after 6000 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 23.0; 145.75483870967741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 6100 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 23.0; 148.18368121442126\n",
      "after 6200 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 23.0; 150.6125237191651\n",
      "after 6300 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 24.0; 153.0413662239089\n",
      "after 6400 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 24.0; 155.47020872865275\n",
      "after 6500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 25.0; 157.8990512333966\n",
      "after 6600 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 25.0; 160.3278937381404\n",
      "after 6700 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 25.0; 162.75673624288424\n",
      "after 6800 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 26.0; 165.18557874762809\n",
      "after 6900 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 26.0; 167.61442125237193\n",
      "after 7000 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 27.0; 170.04326375711574\n",
      "after 7100 cycles, input batch went from 3.0 correct to 3.0\n",
      "Median; mean number of images for each category is 27.0; 172.47210626185958\n",
      "after 7200 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 27.0; 174.90094876660342\n",
      "after 7300 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 28.0; 177.32979127134726\n",
      "after 7400 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 28.0; 179.75863377609107\n",
      "after 7500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 28.0; 182.18747628083491\n",
      "after 7600 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 29.0; 184.61631878557876\n",
      "after 7700 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 29.0; 187.04516129032257\n",
      "after 7800 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 29.0; 189.4740037950664\n",
      "after 7900 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 30.0; 191.90284629981025\n",
      "after 8000 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 30.0; 194.3316888045541\n",
      "after 8100 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 30.5; 196.7605313092979\n",
      "after 8200 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 31.0; 199.18937381404174\n",
      "after 8300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 31.0; 201.61821631878558\n",
      "after 8400 cycles, input batch went from 3.0 correct to 3.0\n",
      "Median; mean number of images for each category is 32.0; 204.04705882352943\n",
      "after 8500 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 32.0; 206.47590132827324\n",
      "after 8600 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 33.0; 208.90474383301708\n",
      "after 8700 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 33.0; 211.33358633776092\n",
      "after 8800 cycles, input batch went from 3.0 correct to 3.0\n",
      "Median; mean number of images for each category is 33.0; 213.76242884250473\n",
      "after 8900 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 33.0; 216.19127134724857\n",
      "after 9000 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 34.0; 218.62011385199241\n",
      "after 9100 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 34.0; 221.04895635673626\n",
      "after 9200 cycles, input batch went from 0.0 correct to 3.0\n",
      "Median; mean number of images for each category is 35.0; 223.47779886148007\n",
      "after 9300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 35.0; 225.9066413662239\n",
      "after 9400 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 35.0; 228.33548387096775\n",
      "after 9500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 36.0; 230.76432637571156\n",
      "after 9600 cycles, input batch went from 3.0 correct to 3.0\n",
      "Median; mean number of images for each category is 36.0; 233.1931688804554\n",
      "after 9700 cycles, input batch went from 3.0 correct to 3.0\n",
      "Median; mean number of images for each category is 37.0; 235.62201138519924\n",
      "after 9800 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 37.0; 238.05085388994308\n",
      "after 9900 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 37.0; 240.4796963946869\n",
      "after 10000 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 38.0; 242.90853889943074\n",
      "after 10100 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 38.0; 245.33738140417458\n",
      "after 10200 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 39.0; 247.76622390891842\n",
      "after 10300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 39.0; 250.19506641366223\n",
      "after 10400 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 39.5; 252.62390891840607\n",
      "after 10500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 40.0; 255.0527514231499\n",
      "after 10600 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 40.0; 257.48159392789375\n",
      "after 10700 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 41.0; 259.9104364326376\n",
      "after 10800 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 41.0; 262.3392789373814\n",
      "after 10900 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 41.0; 264.7681214421252\n",
      "after 11000 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 42.0; 267.19696394686906\n",
      "after 11100 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 42.0; 269.6258064516129\n",
      "after 11200 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 42.0; 272.05464895635674\n",
      "after 11300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 43.0; 274.4834914611006\n",
      "after 11400 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 43.0; 276.9123339658444\n",
      "after 11500 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 43.0; 279.3411764705882\n",
      "after 11600 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 44.0; 281.77001897533205\n",
      "after 11700 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 44.0; 284.1988614800759\n",
      "after 11800 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 44.0; 286.62770398481973\n",
      "after 11900 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 45.0; 289.0565464895636\n",
      "after 12000 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 45.0; 291.4853889943074\n",
      "after 12100 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 46.0; 293.91423149905125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 12200 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 46.0; 296.34307400379504\n",
      "after 12300 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 46.0; 298.7719165085389\n",
      "after 12400 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 47.0; 301.2007590132827\n",
      "after 12500 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 47.0; 303.62960151802656\n",
      "after 12600 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 48.0; 306.0584440227704\n",
      "after 12700 cycles, input batch went from 2.0 correct to 2.0\n",
      "Median; mean number of images for each category is 48.0; 308.48728652751424\n",
      "after 12800 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 48.0; 310.9161290322581\n",
      "after 12900 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 49.0; 313.3449715370019\n",
      "after 13000 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 49.0; 315.7738140417457\n",
      "after 13100 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 49.0; 318.20265654648955\n",
      "after 13200 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 50.0; 320.6314990512334\n",
      "after 13300 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 50.0; 323.06034155597723\n",
      "after 13400 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 50.0; 325.4891840607211\n",
      "after 13500 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 51.0; 327.9180265654649\n",
      "after 13600 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 51.0; 330.34686907020875\n",
      "after 13700 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 51.0; 332.77571157495254\n",
      "after 13800 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 52.0; 335.2045540796964\n",
      "after 13900 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 52.0; 337.6333965844402\n",
      "after 14000 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 52.0; 340.06223908918406\n",
      "after 14100 cycles, input batch went from 3.0 correct to 3.0\n",
      "Median; mean number of images for each category is 52.0; 342.4910815939279\n",
      "after 14200 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 53.0; 344.91992409867174\n",
      "after 14300 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 53.0; 347.3487666034156\n",
      "after 14400 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 54.0; 349.77760910815937\n",
      "after 14500 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 54.0; 352.2064516129032\n",
      "after 14600 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 55.0; 354.63529411764705\n",
      "after 14700 cycles, input batch went from 1.0 correct to 1.0\n",
      "Median; mean number of images for each category is 55.0; 357.0641366223909\n",
      "after 14800 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 56.0; 359.49297912713473\n",
      "after 14900 cycles, input batch went from 0.0 correct to 0.0\n",
      "Median; mean number of images for each category is 56.0; 361.9218216318786\n"
     ]
    }
   ],
   "source": [
    "n_cats = 5270\n",
    "\n",
    "#set up placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 180, 180, 3], name='input_images')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, n_cats], name='one_hotted_labels')\n",
    "\n",
    "#set up neurons for 1st convolutional layer\n",
    "W_conv1 = weight_variable([4, 4, 3, 16])\n",
    "b_conv1 = bias_variable([16])\n",
    "\n",
    "#don't know why this step is necessary, but it is\n",
    "x_image = tf.reshape(x, [-1,180,180,3])\n",
    "\n",
    "#do the first convolutional layer, pooling\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_3x3(h_conv1)\n",
    "\n",
    "#do the second convolutional layer, pooling\n",
    "W_conv2 = weight_variable([4, 4, 16, 32]) #4x4 templates, 32 input channels, 64 output channels\n",
    "b_conv2 = bias_variable([32])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#do the second convolutional layer, pooling\n",
    "W_conv3 = weight_variable([4, 4, 32, 64]) #4x4 templates, 32 input channels, 64 output channels\n",
    "b_conv3 = bias_variable([64])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_3x3(h_conv3)\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 10*10*64])\n",
    "\n",
    "#Densely connected layer\n",
    "W_fc1 = weight_variable([10*10*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#readout layer\n",
    "W = weight_variable([1024, n_cats])\n",
    "b = bias_variable([n_cats])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W) + b\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1, name='argmax_prediction'), tf.argmax(y_,1, name='argmax_actual_val'))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "running_tot = np.zeros(5270, )\n",
    "#Do the training\n",
    "for i in range(15000):\n",
    "    bx, by = next(train_gen)\n",
    "    \n",
    "    running_tot += np.sum(by, 0)\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        #print(i)\n",
    "        init_acc = accuracy.eval(feed_dict={x: bx, y_: by, keep_prob: 1.0})\n",
    "    \n",
    "    train_step.run(feed_dict={x: bx, y_: by, keep_prob: 0.5})\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        #print('After training on those samples:')\n",
    "        aft_acc = accuracy.eval(feed_dict={x: bx, y_: by, keep_prob: 1.0})\n",
    "        print(\"after {} cycles, input batch went from {} correct to {}\".format(i, init_acc*128, aft_acc*128))\n",
    "        print(\"Median; mean number of images for each category is {}; {}\".format(np.median(running_tot), np.mean(running_tot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx, vy = next(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, \"trained_model_1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for k in range(100):\n",
    "    vx, vy = next(val_gen)\n",
    "    acc.append(accuracy.eval(feed_dict={x: vx, y_: vy, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.eval(feed_dict={x: vx, y_: vy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\", input_shape=(180, 180, 3)))\n",
    "#model.add(MaxPooling2D())\n",
    "#model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "#model.add(MaxPooling2D())\n",
    "#model.add(Conv2D(128, 3, padding=\"same\", activation=\"relu\"))\n",
    "#model.add(MaxPooling2D())\n",
    "#model.add(GlobalAveragePooling2D())\n",
    "#model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15*15*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_shape=(180, 180, 3)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = pd.read_csv('categories.csv', index_col='category_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 3, 16)\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "16\n",
      "768\n",
      "(16,)\n",
      "1\n",
      "16\n",
      "16\n",
      "(4, 4, 16, 32)\n",
      "4\n",
      "4\n",
      "4\n",
      "16\n",
      "32\n",
      "8192\n",
      "(32,)\n",
      "1\n",
      "32\n",
      "32\n",
      "(4, 4, 32, 64)\n",
      "4\n",
      "4\n",
      "4\n",
      "32\n",
      "64\n",
      "32768\n",
      "(64,)\n",
      "1\n",
      "64\n",
      "64\n",
      "(14400, 1024)\n",
      "2\n",
      "14400\n",
      "1024\n",
      "14745600\n",
      "(1024,)\n",
      "1\n",
      "1024\n",
      "1024\n",
      "(1024, 5270)\n",
      "2\n",
      "1024\n",
      "5270\n",
      "5396480\n",
      "(5270,)\n",
      "1\n",
      "5270\n",
      "5270\n",
      "(4, 4, 3, 16)\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "16\n",
      "768\n",
      "(16,)\n",
      "1\n",
      "16\n",
      "16\n",
      "(4, 4, 16, 32)\n",
      "4\n",
      "4\n",
      "4\n",
      "16\n",
      "32\n",
      "8192\n",
      "(32,)\n",
      "1\n",
      "32\n",
      "32\n",
      "(4, 4, 32, 64)\n",
      "4\n",
      "4\n",
      "4\n",
      "32\n",
      "64\n",
      "32768\n",
      "(64,)\n",
      "1\n",
      "64\n",
      "64\n",
      "(14400, 1024)\n",
      "2\n",
      "14400\n",
      "1024\n",
      "14745600\n",
      "(1024,)\n",
      "1\n",
      "1024\n",
      "1024\n",
      "(1024, 5270)\n",
      "2\n",
      "1024\n",
      "5270\n",
      "5396480\n",
      "(5270,)\n",
      "1\n",
      "5270\n",
      "5270\n",
      "40380428\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt.loc[[1000003400, 1000003404]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt['category_level1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(nt['category_level1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.fit(range(49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.fit(le.transform(nt['category_level1'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ohe.fit_transform(le.transform(nt['category_level1']).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vs = nt['category_level1'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_ls = le.transform(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = np.zeros((100, 49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    ohe[i, vs_ls[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
